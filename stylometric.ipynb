{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27gur\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 27.4MB/s]                    \n",
      "2024-07-02 21:28:44 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:28:44 INFO: Downloading default packages for language: de (German) ...\n",
      "2024-07-02 21:28:47 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\de\\default.zip\n",
      "2024-07-02 21:28:54 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 24.7MB/s]                    \n",
      "2024-07-02 21:28:54 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:28:54 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-07-02 21:28:55 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\en\\default.zip\n",
      "2024-07-02 21:28:59 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 30.6MB/s]                    \n",
      "2024-07-02 21:28:59 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:28:59 INFO: Downloading default packages for language: uk (Ukrainian) ...\n",
      "2024-07-02 21:29:00 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\uk\\default.zip\n",
      "2024-07-02 21:29:03 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 3.32MB/s]                    \n",
      "2024-07-02 21:29:03 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:29:03 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.8.0/models/default.zip: 100%|██████████| 642M/642M [01:13<00:00, 8.71MB/s] \n",
      "2024-07-02 21:30:22 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\es\\default.zip\n",
      "2024-07-02 21:30:27 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 27.4MB/s]                    \n",
      "2024-07-02 21:30:27 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:30:27 INFO: Downloading default packages for language: nl (Dutch) ...\n",
      "2024-07-02 21:30:29 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\nl\\default.zip\n",
      "2024-07-02 21:30:32 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 22.6MB/s]                    \n",
      "2024-07-02 21:30:32 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:30:32 INFO: Downloading default packages for language: ca (Catalan) ...\n",
      "2024-07-02 21:30:32 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\ca\\default.zip\n",
      "2024-07-02 21:30:34 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 27.4MB/s]                    \n",
      "2024-07-02 21:30:34 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:30:34 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2024-07-02 21:30:36 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\ru\\default.zip\n",
      "2024-07-02 21:30:40 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 24.1MB/s]                    \n",
      "2024-07-02 21:30:40 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:30:40 INFO: Downloading default packages for language: pt (Portuguese) ...\n",
      "2024-07-02 21:30:41 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\pt\\default.zip\n",
      "2024-07-02 21:30:44 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 23.2MB/s]                    \n",
      "2024-07-02 21:30:44 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:30:44 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-07-02 21:30:45 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\ar\\default.zip\n",
      "2024-07-02 21:30:48 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 24.0MB/s]                    \n",
      "2024-07-02 21:30:48 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:30:48 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-07-02 21:30:48 INFO: Downloading default packages for language: zh-hans (Simplified_Chinese) ...\n",
      "2024-07-02 21:30:50 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\zh-hans\\default.zip\n",
      "2024-07-02 21:30:56 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 24.0MB/s]                    \n",
      "2024-07-02 21:30:56 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:30:56 INFO: Downloading default packages for language: cs (Czech) ...\n",
      "2024-07-02 21:30:57 INFO: File exists: C:\\Users\\27gur\\stanza_resources\\cs\\default.zip\n",
      "2024-07-02 21:30:58 INFO: Finished downloading models and saved to C:\\Users\\27gur\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import stopwordsiso as stopwords\n",
    "import nltk\n",
    "\n",
    "languages = ['de', 'en', 'uk', 'es', 'nl', 'ca', 'ru', 'pt', 'ar', 'zh', 'cs']\n",
    "for lang in languages:\n",
    "    stanza.download(lang)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 21:30:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 32.0MB/s]                    \n",
      "2024-07-02 21:30:59 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:02 INFO: Loading these models for language: de (German):\n",
      "===============================\n",
      "| Processor    | Package      |\n",
      "-------------------------------\n",
      "| tokenize     | gsd          |\n",
      "| mwt          | gsd          |\n",
      "| pos          | gsd_charlm   |\n",
      "| lemma        | gsd_nocharlm |\n",
      "| constituency | spmrl_charlm |\n",
      "| depparse     | gsd_charlm   |\n",
      "| sentiment    | sb10k_charlm |\n",
      "| ner          | germeval2014 |\n",
      "===============================\n",
      "\n",
      "2024-07-02 21:31:02 INFO: Using device: cuda\n",
      "2024-07-02 21:31:02 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:07 INFO: Loading: mwt\n",
      "2024-07-02 21:31:07 INFO: Loading: pos\n",
      "2024-07-02 21:31:07 INFO: Loading: lemma\n",
      "2024-07-02 21:31:07 INFO: Loading: constituency\n",
      "2024-07-02 21:31:08 INFO: Loading: depparse\n",
      "2024-07-02 21:31:09 INFO: Loading: sentiment\n",
      "2024-07-02 21:31:09 INFO: Loading: ner\n",
      "2024-07-02 21:31:10 INFO: Done loading processors!\n",
      "2024-07-02 21:31:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza de works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 27.4MB/s]                    \n",
      "2024-07-02 21:31:10 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:12 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-07-02 21:31:12 INFO: Using device: cuda\n",
      "2024-07-02 21:31:12 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:12 INFO: Loading: mwt\n",
      "2024-07-02 21:31:12 INFO: Loading: pos\n",
      "2024-07-02 21:31:13 INFO: Loading: lemma\n",
      "2024-07-02 21:31:13 INFO: Loading: constituency\n",
      "2024-07-02 21:31:14 INFO: Loading: depparse\n",
      "2024-07-02 21:31:14 INFO: Loading: sentiment\n",
      "2024-07-02 21:31:14 INFO: Loading: ner\n",
      "2024-07-02 21:31:15 INFO: Done loading processors!\n",
      "2024-07-02 21:31:15 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza en works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 25.5MB/s]                    \n",
      "2024-07-02 21:31:15 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:17 INFO: Loading these models for language: uk (Ukrainian):\n",
      "===========================\n",
      "| Processor | Package     |\n",
      "---------------------------\n",
      "| tokenize  | iu          |\n",
      "| mwt       | iu          |\n",
      "| pos       | iu_charlm   |\n",
      "| lemma     | iu_nocharlm |\n",
      "| depparse  | iu_charlm   |\n",
      "| ner       | languk      |\n",
      "===========================\n",
      "\n",
      "2024-07-02 21:31:17 INFO: Using device: cuda\n",
      "2024-07-02 21:31:17 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:17 INFO: Loading: mwt\n",
      "2024-07-02 21:31:17 INFO: Loading: pos\n",
      "2024-07-02 21:31:18 INFO: Loading: lemma\n",
      "2024-07-02 21:31:18 INFO: Loading: depparse\n",
      "2024-07-02 21:31:18 INFO: Loading: ner\n",
      "2024-07-02 21:31:19 INFO: Done loading processors!\n",
      "2024-07-02 21:31:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza uk works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 25.6MB/s]                    \n",
      "2024-07-02 21:31:19 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:21 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2024-07-02 21:31:21 INFO: Using device: cuda\n",
      "2024-07-02 21:31:21 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:21 INFO: Loading: mwt\n",
      "2024-07-02 21:31:21 INFO: Loading: pos\n",
      "2024-07-02 21:31:22 INFO: Loading: lemma\n",
      "2024-07-02 21:31:22 INFO: Loading: constituency\n",
      "2024-07-02 21:31:23 INFO: Loading: depparse\n",
      "2024-07-02 21:31:24 INFO: Loading: sentiment\n",
      "2024-07-02 21:31:24 INFO: Loading: ner\n",
      "2024-07-02 21:31:25 INFO: Done loading processors!\n",
      "2024-07-02 21:31:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza es works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 23.5MB/s]                    \n",
      "2024-07-02 21:31:25 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:27 INFO: Loading these models for language: nl (Dutch):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | alpino          |\n",
      "| mwt       | alpino          |\n",
      "| pos       | alpino_charlm   |\n",
      "| lemma     | alpino_nocharlm |\n",
      "| depparse  | alpino_charlm   |\n",
      "| ner       | conll02         |\n",
      "===============================\n",
      "\n",
      "2024-07-02 21:31:27 INFO: Using device: cuda\n",
      "2024-07-02 21:31:27 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:27 INFO: Loading: mwt\n",
      "2024-07-02 21:31:27 INFO: Loading: pos\n",
      "2024-07-02 21:31:28 INFO: Loading: lemma\n",
      "2024-07-02 21:31:28 INFO: Loading: depparse\n",
      "2024-07-02 21:31:29 INFO: Loading: ner\n",
      "2024-07-02 21:31:30 INFO: Done loading processors!\n",
      "2024-07-02 21:31:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza nl works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 25.6MB/s]                    \n",
      "2024-07-02 21:31:30 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:31 INFO: Loading these models for language: ca (Catalan):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | ancora          |\n",
      "| mwt       | ancora          |\n",
      "| pos       | ancora_nocharlm |\n",
      "| lemma     | ancora_nocharlm |\n",
      "| depparse  | ancora_nocharlm |\n",
      "===============================\n",
      "\n",
      "2024-07-02 21:31:31 INFO: Using device: cuda\n",
      "2024-07-02 21:31:31 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:31 INFO: Loading: mwt\n",
      "2024-07-02 21:31:31 INFO: Loading: pos\n",
      "2024-07-02 21:31:32 INFO: Loading: lemma\n",
      "2024-07-02 21:31:32 INFO: Loading: depparse\n",
      "2024-07-02 21:31:33 INFO: Done loading processors!\n",
      "2024-07-02 21:31:33 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza ca works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 23.6MB/s]                    \n",
      "2024-07-02 21:31:33 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:36 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "| depparse  | syntagrus_charlm   |\n",
      "| ner       | wikiner            |\n",
      "==================================\n",
      "\n",
      "2024-07-02 21:31:36 INFO: Using device: cuda\n",
      "2024-07-02 21:31:36 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:36 INFO: Loading: pos\n",
      "2024-07-02 21:31:37 INFO: Loading: lemma\n",
      "2024-07-02 21:31:37 INFO: Loading: depparse\n",
      "2024-07-02 21:31:39 INFO: Loading: ner\n",
      "2024-07-02 21:31:45 INFO: Done loading processors!\n",
      "2024-07-02 21:31:45 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza ru works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 27.4MB/s]                    \n",
      "2024-07-02 21:31:45 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:47 INFO: Loading these models for language: pt (Portuguese):\n",
      "==================================\n",
      "| Processor    | Package         |\n",
      "----------------------------------\n",
      "| tokenize     | bosque          |\n",
      "| mwt          | bosque          |\n",
      "| pos          | bosque_charlm   |\n",
      "| lemma        | bosque_nocharlm |\n",
      "| constituency | cintil_charlm   |\n",
      "| depparse     | bosque_charlm   |\n",
      "==================================\n",
      "\n",
      "2024-07-02 21:31:47 INFO: Using device: cuda\n",
      "2024-07-02 21:31:47 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:47 INFO: Loading: mwt\n",
      "2024-07-02 21:31:47 INFO: Loading: pos\n",
      "2024-07-02 21:31:48 INFO: Loading: lemma\n",
      "2024-07-02 21:31:48 INFO: Loading: constituency\n",
      "2024-07-02 21:31:49 INFO: Loading: depparse\n",
      "2024-07-02 21:31:50 INFO: Done loading processors!\n",
      "2024-07-02 21:31:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza pt works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 27.4MB/s]                    \n",
      "2024-07-02 21:31:50 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:51 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-07-02 21:31:51 INFO: Using device: cuda\n",
      "2024-07-02 21:31:51 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:51 INFO: Loading: mwt\n",
      "2024-07-02 21:31:51 INFO: Loading: pos\n",
      "2024-07-02 21:31:52 INFO: Loading: lemma\n",
      "2024-07-02 21:31:53 INFO: Loading: depparse\n",
      "2024-07-02 21:31:54 INFO: Loading: ner\n",
      "2024-07-02 21:31:55 INFO: Done loading processors!\n",
      "2024-07-02 21:31:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza ar works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 16.7MB/s]                    \n",
      "2024-07-02 21:31:55 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:31:55 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-07-02 21:31:58 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "===================================\n",
      "| Processor    | Package          |\n",
      "-----------------------------------\n",
      "| tokenize     | gsdsimp          |\n",
      "| pos          | gsdsimp_charlm   |\n",
      "| lemma        | gsdsimp_nocharlm |\n",
      "| constituency | ctb-51_charlm    |\n",
      "| depparse     | gsdsimp_charlm   |\n",
      "| sentiment    | ren_charlm       |\n",
      "| ner          | ontonotes        |\n",
      "===================================\n",
      "\n",
      "2024-07-02 21:31:58 INFO: Using device: cuda\n",
      "2024-07-02 21:31:58 INFO: Loading: tokenize\n",
      "2024-07-02 21:31:58 INFO: Loading: pos\n",
      "2024-07-02 21:31:59 INFO: Loading: lemma\n",
      "2024-07-02 21:31:59 INFO: Loading: constituency\n",
      "2024-07-02 21:32:02 INFO: Loading: depparse\n",
      "2024-07-02 21:32:04 INFO: Loading: sentiment\n",
      "2024-07-02 21:32:06 INFO: Loading: ner\n",
      "2024-07-02 21:32:10 INFO: Done loading processors!\n",
      "2024-07-02 21:32:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza zh works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 384kB [00:00, 22.4MB/s]                    \n",
      "2024-07-02 21:32:10 INFO: Downloaded file to C:\\Users\\27gur\\stanza_resources\\resources.json\n",
      "2024-07-02 21:32:11 INFO: Loading these models for language: cs (Czech):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | pdt          |\n",
      "| mwt       | pdt          |\n",
      "| pos       | pdt_nocharlm |\n",
      "| lemma     | pdt_nocharlm |\n",
      "| depparse  | pdt_nocharlm |\n",
      "============================\n",
      "\n",
      "2024-07-02 21:32:11 INFO: Using device: cuda\n",
      "2024-07-02 21:32:11 INFO: Loading: tokenize\n",
      "2024-07-02 21:32:11 INFO: Loading: mwt\n",
      "2024-07-02 21:32:11 INFO: Loading: pos\n",
      "2024-07-02 21:32:17 INFO: Loading: lemma\n",
      "2024-07-02 21:32:17 INFO: Loading: depparse\n",
      "2024-07-02 21:32:18 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza cs works\n"
     ]
    }
   ],
   "source": [
    "stanza_pipelines={}\n",
    "for lang in languages:\n",
    "    nlp = stanza.Pipeline(lang)\n",
    "    print(f\"stanza {lang} works\")\n",
    "    stanza_pipelines[lang]=nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stylometric feature extraction\n",
    "\n",
    "# Lexical Features\n",
    "# word count\n",
    "\n",
    "\n",
    "class StylometricFeatures:\n",
    "    def __init__(self, text, lang):\n",
    "        self.text = text\n",
    "        self.lang = lang\n",
    "        self.doc = stanza_pipelines[lang](text)\n",
    "        self.first_person_pronouns = {\n",
    "            \"en\": {\"I\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"},\n",
    "            \"es\": {\"yo\", \"me\", \"mi\", \"mío\", \"mía\", \"nosotros\", \"nosotras\", \"nos\", \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\"},\n",
    "            \"ru\": {\"я\", \"меня\", \"мне\", \"мной\", \"мой\", \"моего\", \"моему\", \"моим\", \"моём\", \"моя\", \"моей\", \"мою\", \"мое\", \"мои\", \"моих\", \"моим\", \"моими\", \"мы\", \"нас\", \"нам\", \"нами\", \"наш\", \"нашего\", \"нашему\", \"нашим\", \"наше\", \"наша\", \"нашей\", \"нашу\", \"наши\", \"наших\", \"нашими\"},\n",
    "            \"nl\": {\"ik\", \"mij\", \"mijn\", \"mijne\", \"wij\", \"we\", \"ons\", \"onze\"},\n",
    "            \"ca\": {\"jo\", \"em\", \"m'\", \"meu\", \"meva\", \"meus\", \"meves\", \"nosaltres\", \"ens\", \"nostre\", \"nostra\", \"nostres\"},\n",
    "            \"cs\": {\"já\", \"mě\", \"mně\", \"mnou\", \"můj\", \"moje\", \"má\", \"moji\", \"mé\", \"mého\", \"mému\", \"mými\", \"my\", \"nás\", \"nám\", \"námi\", \"náš\", \"naše\", \"naší\", \"našem\", \"našeho\", \"našemu\", \"našimi\"},\n",
    "            \"de\": {\"ich\", \"mich\", \"mir\", \"mein\", \"meine\", \"meiner\", \"meines\", \"wir\", \"uns\", \"unser\", \"unsere\", \"unserer\", \"unseres\"},\n",
    "            \"zh\": {\"我\", \"我的\", \"我们\", \"我们的\"},\n",
    "            \"pt\": {\"eu\", \"me\", \"mim\", \"meu\", \"minha\", \"meus\", \"minhas\", \"nós\", \"nos\", \"nosso\", \"nossa\", \"nossos\", \"nossas\"},\n",
    "            \"ar\": {\"أنا\", \"لي\", \"لنا\", \"نحن\", \"إياي\", \"إيانا\"},\n",
    "            \"uk\": {\"я\", \"мене\", \"мені\", \"мною\", \"мій\", \"моє\", \"моя\", \"мої\", \"моїх\", \"моїм\", \"моєю\", \"ми\", \"нас\", \"нам\", \"нами\", \"наш\", \"наше\", \"наша\", \"наші\", \"наших\", \"нашими\"}\n",
    "        }\n",
    "        self.tokens = [\n",
    "            word.text for sent in self.doc.sentences for word in sent.words]\n",
    "        self.unique_tokens = set(self.tokens)\n",
    "        self.word_count = len(self.tokens)  # Cached word count\n",
    "        # Cached unique word count\n",
    "        self.unique_word_count = len(self.unique_tokens)\n",
    "        self.char_count = sum(len(word)\n",
    "                              for word in self.tokens)  # Cached character count\n",
    "        self.sentence_count = len(self.doc.sentences)  # Cached sentence count\n",
    "        self.punctuation_marks = [\n",
    "            '.', ',', ';', ':', '!', '?', '-', '—', '_', '(', ')', '[', ']', '{', '}', '\"', \"'\", '‘', '’', '“', '”',\n",
    "            '«', '»', '‹', '›', '/', '\\\\', '|', '...', '¡', '¿', '*', '&', '#', '@', '˙', '•', '%', '+', '=', '<', '>',\n",
    "            '°', '¬', '^', '¨', '´', '`', '·', '•', '¤', '¢', '£', '¥', '€', '©', '®', '™', '§', '¶', '×', '÷', '±'\n",
    "        ]\n",
    "        self.stop_words = set(stopwords.stopwords(lang))\n",
    "\n",
    "    @property\n",
    "    def avg_word_length(self):\n",
    "        return self.char_count / self.word_count\n",
    "\n",
    "    @property\n",
    "    def ttr(self):\n",
    "        return self.unique_word_count / self.word_count\n",
    "\n",
    "    @property\n",
    "    def hapax_legomenon(self):\n",
    "        word_freq = nltk.FreqDist(self.tokens)\n",
    "        hapax_legomenon = sum(1 for word in word_freq if word_freq[word] == 1)\n",
    "        return hapax_legomenon / self.word_count if self.word_count else 0\n",
    "\n",
    "    @property\n",
    "    def avg_sentence_length(self):\n",
    "        return self.word_count / self.sentence_count if self.sentence_count else 0\n",
    "\n",
    "    # average sentence complexity\n",
    "    @property\n",
    "    def avg_sentence_complexity(self):\n",
    "        return self.sentence_count / self.word_count if self.word_count else 0\n",
    "\n",
    "    # count of punctuations\n",
    "    @property\n",
    "    def punctuation_count(self):\n",
    "        return sum(word in self.punctuation_marks for word in self.tokens)\n",
    "\n",
    "    @property\n",
    "    def noun_count(self):\n",
    "        return sum(word.upos == 'NOUN' for sent in self.doc.sentences for word in sent.words)\n",
    "\n",
    "    @property\n",
    "    def verb_count(self):\n",
    "        return sum(word.upos == 'VERB' for sent in self.doc.sentences for word in sent.words)\n",
    "\n",
    "    @property\n",
    "    def adjective_count(self):\n",
    "        return sum(word.upos == 'ADJ' for sent in self.doc.sentences for word in sent.words)\n",
    "\n",
    "    @property\n",
    "    def adverb_count(self):\n",
    "        return sum(word.upos == 'ADV' for sent in self.doc.sentences for word in sent.words)\n",
    "\n",
    "    @property\n",
    "    def stopword_count(self):\n",
    "        return sum(word.lower() in self.stop_words for word in self.tokens)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def complex_sentence_count(self):\n",
    "        complex_sentences = 0\n",
    "        for sent in self.doc.sentences:\n",
    "            clause_count = sum(\n",
    "                1 for word in sent.words if word.deprel == 'conj' or word.deprel == 'advcl')\n",
    "            if clause_count > 0:\n",
    "                complex_sentences += 1\n",
    "        return complex_sentences\n",
    "\n",
    "    @property\n",
    "    def question_mark_count(self):\n",
    "        return self.tokens.count('?') + self.tokens.count('¿') + self.tokens.count('؟')\n",
    "\n",
    "    @property\n",
    "    def exclamation_mark_count(self):\n",
    "        return self.tokens.count('!') + self.tokens.count('¡')\n",
    "\n",
    "    @property\n",
    "    def flesch_reading_ease(self):\n",
    "        ASL = self.avg_sentence_length\n",
    "        ASW = self.avg_word_length\n",
    "        return 206.835 - (1.015 * ASL) - (84.6 * ASW)\n",
    "\n",
    "    @property\n",
    "    def gunning_fog_index(self):\n",
    "        return 0.4 * (self.avg_sentence_length + 100 * (self.complex_sentence_count / self.word_count))\n",
    "\n",
    "    @property\n",
    "    def first_person_pronoun_count(self):\n",
    "        first_person_pronouns = set(self.first_person_pronouns[self.lang])\n",
    "        return sum(word.text.lower() in first_person_pronouns for sent in self.doc.sentences for word in sent.words)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def person_entity_count(self):\n",
    "        return sum(ent.type == 'PERSON' for ent in self.doc.ents)\n",
    "\n",
    "    @property\n",
    "    def date_entity_count(self):\n",
    "        return sum(ent.type == 'DATE' for ent in self.doc.ents)\n",
    "\n",
    "\n",
    "    def calculate_uniqueness_stanza(self):\n",
    "        # Use pre-cached tokens if already available\n",
    "        bi_grams = nltk.bigrams(self.tokens)\n",
    "        tri_grams = nltk.trigrams(self.tokens)\n",
    "\n",
    "        unique_bigrams = len(set(bi_grams))\n",
    "        unique_trigrams = len(set(tri_grams))\n",
    "        \n",
    "        # Since set consumes the generator, recount by regenerating\n",
    "        total_bigrams = sum(1 for _ in nltk.bigrams(self.tokens))\n",
    "        total_trigrams = sum(1 for _ in nltk.trigrams(self.tokens))\n",
    "\n",
    "        bigram_uniqueness = unique_bigrams / total_bigrams if total_bigrams > 0 else 0\n",
    "        trigram_uniqueness = unique_trigrams / total_trigrams if total_trigrams > 0 else 0\n",
    "\n",
    "        return bigram_uniqueness, trigram_uniqueness\n",
    "\n",
    "\n",
    "    def calculate_syntax_variety(self):\n",
    "        pos_tags = set(word.upos for sent in self.doc.sentences for word in sent.words)\n",
    "        return len(pos_tags)\n",
    "    \n",
    "    def get_features(self):\n",
    "        return {\n",
    "            'word_count': self.word_count,\n",
    "            'unique_word_count': self.unique_word_count,\n",
    "            'char_count': self.char_count,\n",
    "            'avg_word_length': self.avg_word_length,\n",
    "            'ttr': self.ttr,\n",
    "            'hapax_legomenon': self.hapax_legomenon,\n",
    "            'avg_sentence_length': self.avg_sentence_length,\n",
    "            'avg_sentence_complexity': self.avg_sentence_complexity,\n",
    "            'punctuation_count': self.punctuation_count,\n",
    "            'noun_count': self.noun_count,\n",
    "            'stopword_count': self.stopword_count,\n",
    "            'verb_count': self.verb_count,\n",
    "            'adj_count': self.adjective_count,\n",
    "            'adv_count': self.adverb_count,\n",
    "            'complex_sentence_count': self.complex_sentence_count,\n",
    "            'question_mark_count': self.question_mark_count,\n",
    "            'exclamation_mark_count': self.exclamation_mark_count,\n",
    "            'flesch_reading_ease': self.flesch_reading_ease,\n",
    "            'gunning_fog_index': self.gunning_fog_index,\n",
    "            'first_person_pronoun_count': self.first_person_pronoun_count,\n",
    "            'person_entity_count': self.person_entity_count,\n",
    "            'date_entity_count': self.date_entity_count,\n",
    "            'bigram_uniqueness': self.calculate_uniqueness_stanza()[0],\n",
    "            'trigram_uniqueness': self.calculate_uniqueness_stanza()[1],\n",
    "            'syntax_variety': self.calculate_syntax_variety()\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>multi_label</th>\n",
       "      <th>split</th>\n",
       "      <th>language</th>\n",
       "      <th>length</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beginners bbq class taking place in missoula! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>train</td>\n",
       "      <td>en</td>\n",
       "      <td>130</td>\n",
       "      <td>c4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foil plaid lycra and spandex shortall with met...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>train</td>\n",
       "      <td>en</td>\n",
       "      <td>29</td>\n",
       "      <td>c4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how many backlinks per day for new site? discu...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>train</td>\n",
       "      <td>en</td>\n",
       "      <td>187</td>\n",
       "      <td>c4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the denver board of education opened the 2017-...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>train</td>\n",
       "      <td>en</td>\n",
       "      <td>164</td>\n",
       "      <td>c4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bangalore cy junction sbc to gondia junction g...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>train</td>\n",
       "      <td>en</td>\n",
       "      <td>63</td>\n",
       "      <td>c4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label multi_label  \\\n",
       "0  beginners bbq class taking place in missoula! ...      0       human   \n",
       "1  foil plaid lycra and spandex shortall with met...      0       human   \n",
       "2  how many backlinks per day for new site? discu...      0       human   \n",
       "3  the denver board of education opened the 2017-...      0       human   \n",
       "4  bangalore cy junction sbc to gondia junction g...      0       human   \n",
       "\n",
       "   split language  length source  \n",
       "0  train       en     130     c4  \n",
       "1  train       en      29     c4  \n",
       "2  train       en     187     c4  \n",
       "3  train       en     164     c4  \n",
       "4  train       en      63     c4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dataset/c4train.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 92/34000 [02:56<18:02:43,  1.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39mget_features()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Apply the function to the DataFrame with a progress bar\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_stylometric_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexpand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m, in \u001b[0;36mapply_stylometric_features\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_stylometric_features\u001b[39m(row):\n\u001b[1;32m----> 6\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mStylometricFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlanguage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39mget_features()\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mStylometricFeatures.__init__\u001b[1;34m(self, text, lang)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m=\u001b[39m lang\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[43mstanza_pipelines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_person_pronouns \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mour\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mours\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmío\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmía\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnosotros\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnosotras\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnos\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuestro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuestra\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuestros\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuestras\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muk\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mя\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмене\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмені\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмною\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмій\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоє\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоя\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмої\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоїх\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоїм\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоєю\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mми\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнас\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнам\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнами\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаш\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаше\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаша\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаші\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаших\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнашими\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     24\u001b[0m }\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     word\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent\u001b[38;5;241m.\u001b[39mwords]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stanza\\pipeline\\core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stanza\\pipeline\\core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[0;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[1;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stanza\\pipeline\\ner_processor.py:114\u001b[0m, in \u001b[0;36mNERProcessor.process\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m    112\u001b[0m         preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[1;32m--> 114\u001b[0m             preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m         all_preds\u001b[38;5;241m.\u001b[39mappend(preds)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# for each sentence, gather a list of predictions\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# merge those predictions into a single list\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# earlier models will have precedence\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stanza\\models\\ner\\trainer.py:139\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, batch, unsort)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m#batch_size = word.size(0)\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m _, logits, trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_orig_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharoffsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_orig_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# decode\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# TODO: might need to decode multiple columns of output for\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# models with multiple layers\u001b[39;00m\n\u001b[0;32m    144\u001b[0m trans \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m trans]\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stanza\\models\\ner\\model.py:256\u001b[0m, in \u001b[0;36mNERTagger.forward\u001b[1;34m(self, sentences, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# the tag_mask lets us avoid backprop on a blank tag\u001b[39;00m\n\u001b[0;32m    255\u001b[0m tag_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meq(tags[:, :, idx], EMPTY_ID)\n\u001b[1;32m--> 256\u001b[0m next_loss, next_trans \u001b[38;5;241m=\u001b[39m \u001b[43mcrit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbitwise_or\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m next_loss\n\u001b[0;32m    258\u001b[0m logits\u001b[38;5;241m.\u001b[39mappend(next_logits)\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stanza\\models\\common\\crf.py:36\u001b[0m, in \u001b[0;36mCRFLoss.forward\u001b[1;34m(self, inputs, masks, tag_indices)\u001b[0m\n\u001b[0;32m     34\u001b[0m unary_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrf_unary_score(inputs, masks, tag_indices, input_bs, input_sl, input_nc)\n\u001b[0;32m     35\u001b[0m binary_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrf_binary_score(inputs, masks, tag_indices, input_bs, input_sl, input_nc)\n\u001b[1;32m---> 36\u001b[0m log_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf_log_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m unary_scores \u001b[38;5;241m+\u001b[39m binary_scores \u001b[38;5;241m-\u001b[39m log_norm \u001b[38;5;66;03m# batch_size\u001b[39;00m\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39mlog_likelihood)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stanza\\models\\common\\crf.py:97\u001b[0m, in \u001b[0;36mCRFLoss.crf_log_norm\u001b[1;34m(self, inputs, masks, tag_indices)\u001b[0m\n\u001b[0;32m     95\u001b[0m     m \u001b[38;5;241m=\u001b[39m rest_masks[:,i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(new_alphas) \u001b[38;5;66;03m# bs x nc, 1 for padding idx\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# apply masks\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     new_alphas\u001b[38;5;241m.\u001b[39mmasked_scatter_(m, \u001b[43malphas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     98\u001b[0m     alphas \u001b[38;5;241m=\u001b[39m new_alphas\n\u001b[0;32m     99\u001b[0m log_norm \u001b[38;5;241m=\u001b[39m log_sum_exp(alphas, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Assuming df is your DataFrame and it contains 'text' and 'lang' columns\n",
    "def apply_stylometric_features(row):\n",
    "    features = StylometricFeatures(text=row['text'], lang=row['language'])\n",
    "    return features.get_features()\n",
    "\n",
    "# Apply the function to the DataFrame with a progress bar\n",
    "df_features = df.progress_apply(apply_stylometric_features, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features:   0%|          | 0/74081 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features:   0%|          | 4/74081 [00:38<197:21:49,  9.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m lang \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Create an instance of the StylometricFeatures class\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mStylometricFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate the features\u001b[39;00m\n\u001b[0;32m     19\u001b[0m results\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mword_count\n",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m, in \u001b[0;36mStylometricFeatures.__init__\u001b[1;34m(self, text, lang)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m=\u001b[39m lang\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[43mstanza_pipelines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_person_pronouns \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mour\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mours\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmío\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmía\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnosotros\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnosotras\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnos\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuestro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuestra\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuestros\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuestras\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muk\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mя\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмене\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмені\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмною\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмій\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоє\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоя\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмої\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоїх\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоїм\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mмоєю\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mми\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнас\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнам\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнами\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаш\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаше\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаша\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаші\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнаших\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mнашими\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     24\u001b[0m     word\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent\u001b[38;5;241m.\u001b[39mwords]\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\stanza\\pipeline\\core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\stanza\\pipeline\\core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[0;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[1;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\stanza\\pipeline\\tokenize_processor.py:104\u001b[0m, in \u001b[0;36mTokenizeProcessor.process\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# get dict data\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 104\u001b[0m     _, _, _, document \u001b[38;5;241m=\u001b[39m \u001b[43moutput_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43morig_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mno_ssplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno_ssplit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mpostprocessor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# replace excessively long tokens with <UNK> to avoid downstream GPU memory issues in POS\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m document:\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\stanza\\models\\tokenization\\utils.py:324\u001b[0m, in \u001b[0;36moutput_predictions\u001b[1;34m(output_file, trainer, data_generator, vocab, mwt_dict, max_seqlen, orig_text, no_ssplit, use_regex_tokens, num_workers, postprocessor)\u001b[0m\n\u001b[0;32m    321\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    322\u001b[0m max_seqlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1000\u001b[39m, max_seqlen)\n\u001b[1;32m--> 324\u001b[0m all_preds, all_raw \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_regex_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m use_la_ittb_shorthand \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshorthand\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mla_ittb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    327\u001b[0m skip_newline \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip_newline\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\stanza\\models\\tokenization\\utils.py:278\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(trainer, data_generator, batch_size, max_seqlen, use_regex_tokens, num_workers)\u001b[0m\n\u001b[0;32m    276\u001b[0m en \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(ens)\n\u001b[0;32m    277\u001b[0m batch1 \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m][:, :en], batch[\u001b[38;5;241m1\u001b[39m][:, :en], batch[\u001b[38;5;241m2\u001b[39m][:, :en], [x[:en] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m--> 278\u001b[0m pred1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch1\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sentences):\n\u001b[0;32m    281\u001b[0m     sentbreaks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((pred1[j] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m (pred1[j] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:63\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     60\u001b[0m units \u001b[38;5;241m=\u001b[39m units\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     61\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 63\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\stanza\\models\\tokenization\\model.py:52\u001b[0m, in \u001b[0;36mTokenizer.forward\u001b[1;34m(self, x, feats)\u001b[0m\n\u001b[0;32m     47\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_feat(feats)\n\u001b[0;32m     50\u001b[0m emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([emb, feats], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m inp, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv_res\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     conv_input \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\27gur\\.conda\\envs\\daigt\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:911\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 911\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    915\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the DataFrame (Assuming it is already loaded if not, load it here)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Preallocate a DataFrame for the results to avoid appending to lists\n",
    "results = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Initialize tqdm over the DataFrame rows\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Calculating features\"):\n",
    "    text = row['text']\n",
    "    lang = row['language']\n",
    "    \n",
    "    # Create an instance of the StylometricFeatures class\n",
    "    features = StylometricFeatures(text, lang)\n",
    "    \n",
    "    # Calculate the features\n",
    "    results.at[index, 'word_count'] = features.word_count\n",
    "    results.at[index, 'unique_word_count'] = features.unique_word_count\n",
    "    results.at[index, 'char_count'] = features.char_count\n",
    "    results.at[index, 'avg_word_length'] = features.avg_word_length\n",
    "    results.at[index, 'ttr'] = features.ttr\n",
    "    results.at[index, 'hapax_legomenon'] = features.hapax_legomenon\n",
    "    results.at[index, 'sentence_count'] = features.sentence_count\n",
    "    results.at[index, 'avg_sentence_length'] = features.avg_sentence_length\n",
    "    results.at[index, 'avg_sentence_complexity'] = features.avg_sentence_complexity\n",
    "    results.at[index, 'punctuation_count'] = features.punctuation_count\n",
    "    results.at[index, 'noun_count'] = features.noun_count\n",
    "    results.at[index, 'stopword_count'] = features.stopword_count\n",
    "    results.at[index, 'verb_count'] = features.verb_count\n",
    "    results.at[index, 'adj_count'] = features.adjective_count\n",
    "    results.at[index, 'adv_count'] = features.adverb_count\n",
    "    results.at[index, 'complex_sentence_count'] = features.complex_sentence_count\n",
    "    results.at[index, 'question_mark_count'] = features.question_mark_count\n",
    "    results.at[index, 'exclamation_mark_count'] = features.exclamation_mark_count\n",
    "    results.at[index, 'flesch_reading_ease'] = features.flesch_reading_ease\n",
    "    results.at[index, 'gunning_fog_index'] = features.gunning_fog_index\n",
    "    results.at[index, 'first_person_pronoun_count'] = features.first_person_pronoun_count\n",
    "    results.at[index, 'person_entity_count'] = features.person_entity_count\n",
    "    results.at[index, 'date_entity_count'] = features.date_entity_count\n",
    "    uniqueness_bigram_val, uniqueness_trigram_val = features.calculate_uniqueness_stanza\n",
    "    results.at[index, 'uniqueness_bigram'] = uniqueness_bigram_val\n",
    "    results.at[index, 'uniqueness_trigram'] = uniqueness_trigram_val\n",
    "    results.at[index, 'syntax_variety'] = features.calculate_syntax_variety\n",
    "\n",
    "# Combine the original DataFrame with the calculated results\n",
    "df = pd.concat([df, results], axis=1)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df.to_csv('updated_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming your dataset is stored in the 'df' variable\n",
    "num_parts = 6\n",
    "df_parts = np.array_split(df, num_parts)\n",
    "\n",
    "# Access each part of the divided dataset\n",
    "for i, part in enumerate(df_parts):\n",
    "    print(f\"Part {i+1}:\")\n",
    "    print(part)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
